{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification: an exploration of different representations and learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This practical sessions was produced using [Jupyter](http://jupyter.org). If you are used to it, you can [download the corresponding notebook code from here](TP-TextClassification.ipynb). If not, no problem at all, this is not mandatory: simply proceed as usual in your favorite Python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this practical session is to get yourself acquainted with the different models that can be used for NLP classification tasks and to get some exposure to the different statistical machine learning and deep learning packages.\n",
    "\n",
    "As we have seen in the [second practical session](https://coling.epfl.ch/TP/TP-eval.php), a great example of a highly-biased task is **spam classification**. The goal of the task is to classify whether a given piece of text (e.g., email, sms message) is spam or not. Since there are only 2 classes (spam or not), we call such tasks **binary classification**. However, text classification tasks can also include **multi-label tasks** such as **news article topic prediction**. Following these two tasks, this practical session has 3 sections:\n",
    "\n",
    "1. Brief Data Analysis & Processing:\n",
    "    - What does the label distribution look like?\n",
    "    - Train/test splitting\n",
    "2. Classical Algorithms:\n",
    "    - Bag-of-Words vs. TF-IDF\n",
    "    - filtering: removing stopwords\n",
    "    - Logistic Regression vs. NaiveBayes\n",
    "3. Beyond Feature Engineering:\n",
    "    - LSTM for spam filtering\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "Huge thanks to [Reza](https://github.com/MohammadrezaBanaei/INLP_neural_practical_session) and [Mehmet](https://www.kaggle.com/code/mehmetlaudatekman/lstm-text-classification-pytorch/notebook) for the following inspirations, a big part of the code is recycled from their notebooks!\n",
    "\n",
    "### Content Warning: this exercise's data may contain explicit words.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up your environment\n",
    "\n",
    "While you can download the following packages with `pip` to your computer directly, we recommend **(but not require)** you to use a [virtual environment](https://packaging.python.org/en/latest/guides/installing-using-pip-and-virtual-environments/) to not mess up the package versions for different project. If you'd like to, here is a [quick tutorial](https://docs.google.com/document/d/1D8TapyWrfyWijfrGq5BtYfrAoWmSmgcP-Gx3i2MXWvo/edit) on virtual environments that you can checkout with an EPFL email. \n",
    "\n",
    "Alternatively you can use the EPFL jupyter notebook service [noto](https://noto.epfl.ch/), however you will have to `pip install` some specific packages such as torchtext.\n",
    "\n",
    "1. First make sure you have (a virtual environment (e.g., [venv, virtualenv](https://docs.python.org/3/library/venv.html), [conda](https://docs.conda.io/en/latest/miniconda.html)), and that the environment has) a Python version >= 3.6, per [scikit-learn](https://scikit-learn.org/stable/index.html) and [torch](https://pytorch.org/) requirements. If you are using the a Jupyter Notebook, make sure the interpreter points to the correct `python` executable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Then install the following packages into your venv:\n",
    "\n",
    "```\n",
    "pip install -U ipykernel\n",
    "pip install -U ipywidgets\n",
    "pip install -U pip setuptools wheel\n",
    "pip install -U pandas\n",
    "pip install -U matplotlib\n",
    "pip install -U scikit-learn\n",
    "pip install -U seaborn\n",
    "pip install -U nltk\n",
    "pip install -U torch\n",
    "pip install -U torchtext==0.10.0\n",
    "pip install -U torchdata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Next, import the necessary packages:\n",
    "\n",
    "*Note: If this part of the code hangs, simply restart your kernel and rerun, sometimes importing packages multiple times can create a problem!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Importing necessary packages:\n",
    "# general\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "from collections import Counter\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "print(\"1\")\n",
    "# dataset + processing\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split #, KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "#\n",
    "# from nltk import word_tokenize\n",
    "# from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(\"2\")\n",
    "# classification models\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "print(\"3\")\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# LSTM part's packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy import data # nlp library of Pytorch\n",
    "import warnings as wrn\n",
    "wrn.filterwarnings('ignore')\n",
    "\n",
    "print(\"4\")\n",
    "# 2) Setting the seed:\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "print(\"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Downloading stopwords from NLTK (if you haven't done it before!)\n",
    "nltk_stopwords_downloaded = True\n",
    "if not nltk_stopwords_downloaded:\n",
    "    nltk.download('stopwords')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now you are ready to start the exercises!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Brief Data Analysis & Processing\n",
    "\n",
    "### a) Spam Dataset\n",
    "To solve and evaluate the spam task, we will use the same annotated English sms corpus from  Kaggle as the first practical session. You can download the data [here](https://coling.epfl.ch/TP/spam.csv). Simply put it in the same folder as the notebook you are running. As we have done the analysis and pre-processing in the first practical session, we will quickly run the same steps. Remember that the labels in the spam dataset are ill-balanced, heavily skewed towards the *ham* label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"spam.csv\", header=0, names=['label','sms'], usecols=[0,1])\n",
    "# df.tail()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are 2 types of classes: \"ham\" & \"spam\". Let's take a look at their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace(to_replace={'ham': 0, 'spam': 1})\n",
    "print(\"Label percentages are:\")\n",
    "print(df.label.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to make sure that we don't overfit our models to the data, we split the data into train and test sets. We use the very convenient [**train_test_split**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function from [scikit-learn](https://scikit-learn.org/stable/index.html). The *test_size* parameter allows us to choose what percentage of the data should be in the test set. $x$ is the sms message, while $y$ is the corresponding label to the sms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_random_state = 11\n",
    "X_train_spam, X_test_spam, y_train_spam, y_test_spam = train_test_split(\n",
    "    df['sms'].values,\n",
    "    df['label'].values,\n",
    "    test_size=0.2, \n",
    "    random_state=train_split_random_state\n",
    ")\n",
    "\n",
    "print(\"Spam amount in train set: {} out of {} datapoints\".format((y_train_spam == 1).sum(), len(y_train_spam)))\n",
    "print(\"Spam percentage in train set: {}%\".format(round((y_train_spam == 1).sum() / float(len(y_train_spam)) * 100, 4)))\n",
    "\n",
    "print(\"Size of train set is: \", len(y_train_spam))\n",
    "print(\"Size of test set is: \", len(y_test_spam))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) 20 newsgroup Dataset\n",
    "\n",
    "To solve and evaluate the news topic classification task, we will use the [20 newsgroup dataset](http://qwone.com/~jason/20Newsgroups/) that has 19K articles in 20 different news groups. We download the data through scikit-learn, so you don't have to manually download it. Let's take a look at the label distribution in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(subset='all')\n",
    "print(\"Number of articles: \", len(news.data))\n",
    "print(\"Number of different categories: \", len(news.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the label distribution of the 20 classes, we plot a pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_labels = news.target_names\n",
    "sizes = [Counter(news.target)[i] for i in range(len(news_labels))]\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.pie(sizes, labels=news_labels, autopct='%1.1f%%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What do you notice about the label distribution of the news dataset compared to the spam one? Does this change your plan on which metric to use to evaluate the classifiers we will test in the next section?** \n",
    "\n",
    "*A: TODO - your answer here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "SOLUTION:\n",
    "The labels are not as biased as the spam classification dataset. All classes are roughly uniformly distributed.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also divide this dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_news, X_test_news, y_train_news, y_test_news = train_test_split(\n",
    "    news.data,\n",
    "    news.target, \n",
    "    test_size=0.2, \n",
    "    random_state=train_split_random_state\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Choosing Features & a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering is when NLP task-specific knowledge can come in handy, and make it more likely for a simple classifier to learn the task. This requires us to index tokens and create meaningful representations out of them.\n",
    "\n",
    "First we have to create a vocabulary. Some of the *indexing* themes you have seen in class include:\n",
    "- **tokenization:** splitting the text into units called tokens, which is required before indexing\n",
    "- **stopwords:** common words that can be filtered\n",
    "\n",
    "To represent a document as a vector however, we need more than just indexing, such as a *vector space* that represents the words:\n",
    "- **Bag-of-Words model:** a single document can be considered as a bag of words and how many times each word occured, without caring about the order of the words. The word occurence counting is also called *term frequency*. You can think if this as a vector over all of the vocabulary where the entries are how many times that term has occured.\n",
    "- **TF-IDF:** *term frequencyâ€“inverse document frequency* diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily scikit-learn provides a `Pipeline` [class](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) where we can put in the correct order the vectorizer and classifier. Take as an example the vectorizer TF-IDF and the first classifier you can think of such as the Naive Bayes classifier ([BernouilliNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)). We can do the following to train and predict with the model on a binary task.\n",
    "```\n",
    "Pipeline([('vectorizer', TfidfVectorizer()), ('classifier', BernoulliNB(alpha=0.2))])\n",
    "```\n",
    "\n",
    "Note that for a multi-label problem you can use [MultinomialNB](). In [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), to specify multi-label, you can switch the `multi_class` parameter value from `ovr` (one-vs-rest) to `auto`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_classifier_dict = {\n",
    "    \"bagofwords+binaryNB\": Pipeline([('vectorizer', CountVectorizer()), ('classifier', BernoulliNB(alpha=0.2))]),\n",
    "    \"bagofwords+binaryLogistic\": Pipeline([('vectorizer', CountVectorizer()), ('classifier', LogisticRegression(solver=\"saga\" , multi_class=\"ovr\", max_iter=200))])\n",
    "}\n",
    "\n",
    "news_classifier_dict = {\n",
    "    \"bagofwords+multiNB\": Pipeline([('vectorizer', CountVectorizer()), ('classifier', MultinomialNB(alpha=0.2))]),\n",
    "    \"bagofwords+multiLogistic\": Pipeline([('vectorizer', CountVectorizer()), ('classifier', LogisticRegression(solver=\"saga\" , multi_class=\"auto\", max_iter=200))])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our classifiers we can train and validate them with cross-validation to see if the vectorizer and classifier combination does well on the task. Here we make sure to further separate the train dataset into several train and validation splits. This way the original test set is unused to prevent overfitting during feature engineering and classification algorithm exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_validation_random_state = [1,5,10,15,20]\n",
    "\n",
    "def train(\n",
    "    classifier, \n",
    "    X_train, \n",
    "    y_train, \n",
    "    rnd_state_input , \n",
    "    test_split_size=0.1, \n",
    "):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            test_size=test_split_size,\n",
    "            random_state=rnd_state_input\n",
    "        )\n",
    "        classifier.fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_val)\n",
    "        if rnd_state_input == 5:\n",
    "            print(\"\\t|| k=5 Accuracy: {}% \".format(accuracy_score(y_val, y_pred)))\n",
    "            print(\"\\t|| k=5 Precision: {}% \".format(precision_score(y_val, y_pred, average='macro')))\n",
    "            print(\"\\t|| k=5 Recall: {}% \".format(recall_score(y_val, y_pred, average='macro')))\n",
    "            print(\"\\t|| k=5 F1: {}% \".format(f1_score(y_val, y_pred, average='macro')))\n",
    "        return classifier, classifier.score(X_val, y_val)\n",
    "\n",
    "def plot_confusion_matrix(classifier, X_test, y_test, labels):\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "    confusion_mat = normalize(confusion_mat , axis=1 , norm='l1' )\n",
    "    # Plot confusion_matrix\n",
    "    fig, ax = plt.subplots(figsize=(10,8))\n",
    "    sns.heatmap(confusion_mat, annot=True, cmap = \"flare\", fmt =\"0.2f\", xticklabels=labels, yticklabels=labels)\n",
    "\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train each model on the task and do a k=5 cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_classifier_dict = {\n",
    "    \"bagofwords+binaryNB\": Pipeline([('vectorizer', CountVectorizer()), ('classifier', BernoulliNB(alpha=0.2))]),\n",
    "    \"bagofwords+binaryLogistic\": Pipeline([('vectorizer', CountVectorizer()), ('classifier', LogisticRegression(solver=\"saga\" , multi_class=\"ovr\", max_iter=200))])\n",
    "}\n",
    "\n",
    "news_classifier_dict = {\n",
    "    \"bagofwords+multiNB\": Pipeline([('vectorizer', CountVectorizer()), ('classifier', MultinomialNB(alpha=0.2))]),\n",
    "    \"bagofwords+multiLogistic\": Pipeline([('vectorizer', CountVectorizer()), ('classifier', LogisticRegression(solver=\"saga\" , multi_class=\"auto\", max_iter=200))])\n",
    "}\n",
    "\n",
    "print(\"// Spam Binary Task Evaluation //\")\n",
    "for model_name, model in spam_classifier_dict.items():\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(model_name + \" : \")\n",
    "    all_cross_val_scores = []\n",
    "    for k in train_validation_random_state:\n",
    "        classifier, score = train(\n",
    "            classifier=model, \n",
    "            X_train=X_train_spam, \n",
    "            y_train=y_train_spam, \n",
    "            rnd_state_input=k\n",
    "        )\n",
    "        all_cross_val_scores.append(score)\n",
    "    all_cross_val_scores_np = np.array(all_cross_val_scores)\n",
    "    mean_score = all_cross_val_scores_np.mean()\n",
    "    print(\"Mean accuracy score on spam: \", mean_score)\n",
    "    plot_confusion_matrix(classifier, X_test_spam, y_test_spam, [1,0])\n",
    "\n",
    "print(\"_______________________________________________________________\")\n",
    "print(\"// News Multi-label Task Evaluation //\")\n",
    "for model_name, model in news_classifier_dict.items():\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(model_name + \" : \") \n",
    "    all_cross_val_scores = []\n",
    "    for k in train_validation_random_state:\n",
    "        classifier, score = train(\n",
    "            classifier=model, \n",
    "            X_train=X_train_news, \n",
    "            y_train=y_train_news, \n",
    "            rnd_state_input=k\n",
    "        )\n",
    "        all_cross_val_scores.append(score)\n",
    "    all_cross_val_scores_np = np.array(all_cross_val_scores)\n",
    "    mean_score = all_cross_val_scores_np.mean()\n",
    "    print(\"Mean accuracy score on news: \", mean_score)\n",
    "    plot_confusion_matrix(classifier, X_test_news, y_test_news, news_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: *An open ended question* - Given that these findings are limited to the Bag-of-Words vectorization, what other vectorization methods could you use? What are some additional indexing themes that could help or hurt each task given the preprocessing and analysis we have done in the first section?**\n",
    "\n",
    "*A: TODO - your answer here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "SOLUTION:\n",
    "We can use stopwords filtration to improve the vectorizer, especially for the latter scores on the news task.\n",
    "We could expect TF-IDF to work well for the news class as it has longer documents that relate to each other.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: *An open ended question* - Which model seems to be extremely poorly? Why do you think this might be the case?**\n",
    "\n",
    "*A: TODO - your answer here!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_classifier_dict = {\n",
    "    \"stopwords+tfidf+binaryNB\": Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))), ('classifier', BernoulliNB(alpha=0.005))]),\n",
    "    \"stopwords+tfidf+binaryLogistic\": Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))), ('classifier', LogisticRegression(solver=\"saga\" , multi_class=\"ovr\", max_iter=200))])\n",
    "}\n",
    "\n",
    "news_classifier_dict = {\n",
    "    \"stopwords+tfidf+multiNB\": Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))), ('classifier', MultinomialNB(alpha=0.005))]),\n",
    "    \"stopwords+tfidf+multiLogistic\": Pipeline([('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))), ('classifier', LogisticRegression(solver=\"saga\" , multi_class=\"auto\", max_iter=200))])\n",
    "}\n",
    "\n",
    "print(\"// Spam Binary Task Evaluation //\")\n",
    "for model_name, model in spam_classifier_dict.items():\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(model_name + \" : \")\n",
    "    all_cross_val_scores = []\n",
    "    for k in train_validation_random_state:\n",
    "        classifier, score = train(\n",
    "            classifier=model, \n",
    "            X_train=X_train_spam, \n",
    "            y_train=y_train_spam, \n",
    "            rnd_state_input=k\n",
    "        )\n",
    "        all_cross_val_scores.append(score)\n",
    "    all_cross_val_scores_np = np.array(all_cross_val_scores)\n",
    "    mean_score = all_cross_val_scores_np.mean()\n",
    "    print(\"Mean accuracy score on spam: \", mean_score)\n",
    "    plot_confusion_matrix(classifier, X_test_spam, y_test_spam, [1,0])\n",
    "\n",
    "print(\"_______________________________________________________________\")\n",
    "print(\"// News Multi-label Task Evaluation //\")\n",
    "for model_name, model in news_classifier_dict.items():\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~\")\n",
    "    print(model_name + \" : \") \n",
    "    all_cross_val_scores = []\n",
    "    for k in train_validation_random_state:\n",
    "        classifier, score = train(\n",
    "            classifier=model, \n",
    "            X_train=X_train_news, \n",
    "            y_train=y_train_news, \n",
    "            rnd_state_input=k\n",
    "        )\n",
    "        all_cross_val_scores.append(score)\n",
    "    all_cross_val_scores_np = np.array(all_cross_val_scores)\n",
    "    mean_score = all_cross_val_scores_np.mean()\n",
    "    print(\"Mean accuracy score on news: \", mean_score)\n",
    "    plot_confusion_matrix(classifier, X_test_news, y_test_news, news_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What do you notice in the change of results? Do you find that the feature augmentation of TF-IDF has helped the task you expected it to help? Do you find that the feature augmentaion of TF-IDF has hurt the task you expected it to hurt?**\n",
    "\n",
    "*A: TODO - your answer here!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "SOLUTION:\n",
    "- TF-IDF hurts logistic regression for the spam task. This is most likely because the sentence are short and not so varied.\n",
    "- On the other hand, the news dataset has long documents with a variety of length, that need to be mitigated with TF-IDF.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Beyond feature engineering - LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bag-of-words style of representation combined with a classifier often misses the order of sentences. Given the following sentences, can you see how this may be problematic?\n",
    "\n",
    "> I went to the bank to take a swim.\n",
    "\n",
    "> I went to the bank to withdraw money.\n",
    "\n",
    "The meaning of the token bank is modulated by its context. To overcome this problem you have seen in class that you could learn a vector space representation of the vocabulary, in which word representation are taught to be closer (through a cosine distance objective) according to the context window in which they are used. Even in this situation, the word's distributional semantics are limited by the window size.\n",
    "\n",
    "Instead we can make the classifier take the input text as a sequence. This line of machine learning algorithms are called Recurrent Neural Networks (RNNs). One popular implementation of such algorithms that you will see next week are LSTMs (long-term short-term memory).\n",
    "\n",
    "Let's implement one in the popular deep learning framework pytorch! PyTorch has a text processor subpackage called torchtext that allows for easy indexing of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy',batch_first=True,include_lengths=True)\n",
    "LABEL = data.LabelField(dtype = torch.float,batch_first=True)\n",
    "fields = [(\"type\",LABEL),('text',TEXT)]\n",
    "\n",
    "training_data = data.TabularDataset(\n",
    "    path=\"spam.csv\",\n",
    "    format=\"csv\",\n",
    "    fields=fields,\n",
    "    skip_header=True\n",
    ")\n",
    "print(vars(training_data.examples[0]))\n",
    "\n",
    "train_data,valid_data = training_data.split(\n",
    "    split_ratio=0.75,\n",
    "    random_state=random.seed(42)\n",
    ")\n",
    "TEXT.build_vocab(\n",
    "    train_data,\n",
    "    min_freq=5\n",
    ")\n",
    "\n",
    "LABEL.build_vocab(train_data)\n",
    "print(\"Size of text vocab:\",len(TEXT.vocab))\n",
    "print(\"Size of label vocab:\",len(LABEL.vocab))\n",
    "TEXT.vocab.freqs.most_common(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GPU variable\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "# NOTE: BucketIterator batches the similar length of samples and reduces the need of padding tokens.\n",
    "train_iterator,validation_iterator = data.BucketIterator.splits(\n",
    "    (train_data,valid_data),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    # Sort key is how to sort the samples\n",
    "    sort_key = lambda x:len(x.text),\n",
    "    sort_within_batch = True,\n",
    "    device = device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=len(TEXT.vocab),\n",
    "        embedding_dim=100,\n",
    "        hidden_dim=64,\n",
    "        output_dim=1,\n",
    "        n_layers=2,\n",
    "        bidirectional=True,\n",
    "        dropout=0.2\n",
    "    ):\n",
    "        \n",
    "        super(LSTMClassifier,self).__init__()\n",
    "        \n",
    "        # Embedding layer converts integer sequences to vector sequences\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        \n",
    "        # LSTM layer process the vector sequences \n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers = n_layers,\n",
    "                            bidirectional = bidirectional,\n",
    "                            dropout = dropout,\n",
    "                            batch_first = True\n",
    "                           )\n",
    "        \n",
    "        # Dense layer to predict \n",
    "        self.fc = nn.Linear(hidden_dim * 2,output_dim)\n",
    "        # Prediction activation function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    \n",
    "    def forward(self,text,text_lengths):\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # Thanks to packing, LSTM don't see padding tokens \n",
    "        # and this makes our model better\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(),batch_first=True)\n",
    "        \n",
    "        packed_output,(hidden_state,cell_state) = self.lstm(packed_embedded)\n",
    "        \n",
    "        # Concatenating the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden_state[-2,:,:], hidden_state[-1,:,:]), dim = 1)\n",
    "        \n",
    "        dense_outputs=self.fc(hidden)\n",
    "\n",
    "        #Final activation function\n",
    "        outputs=self.sigmoid(dense_outputs)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "LSTM_model = LSTMClassifier()\n",
    "print(LSTM_model)\n",
    "\n",
    "LSTM_model = LSTM_model.to(device)\n",
    "optimizer = optim.Adam(LSTM_model.parameters(),lr=1e-4)\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(preds)\n",
    "    \n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def evaluate(model,iterator,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    # deactivate the dropouts\n",
    "    model.eval()\n",
    "    \n",
    "    # Sets require_grad flat False\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text,text_lengths = batch.text\n",
    "            \n",
    "            predictions = model(text,text_lengths).squeeze()\n",
    "              \n",
    "            #compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.type)\n",
    "            acc = binary_accuracy(predictions, batch.type)\n",
    "            \n",
    "            #keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def train(model,iterator,optimizer,criterion):\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        # cleaning the cache of optimizer\n",
    "        optimizer.zero_grad()\n",
    "        text,text_lengths = batch.text\n",
    "        \n",
    "        # forward propagation and squeezing\n",
    "        predictions = model(text,text_lengths).squeeze()\n",
    "        \n",
    "        # computing loss / backward propagation\n",
    "        loss = criterion(predictions,batch.type)\n",
    "        loss.backward()\n",
    "        \n",
    "        # accuracy\n",
    "        acc = binary_accuracy(predictions, batch.type)\n",
    "        \n",
    "        # updating params\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    # It'll return the means of loss and accuracy\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUMBER = 25\n",
    "for epoch in range(1,EPOCH_NUMBER+1):\n",
    "    train_loss,train_acc = train(LSTM_model,train_iterator,optimizer,criterion)\n",
    "    valid_loss,valid_acc = evaluate(LSTM_model,validation_iterator,criterion)\n",
    "    \n",
    "    # Showing statistics\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the LSTM can reach a similar performance on this simple spam classification task.\n",
    "We invite you to further investigate how LSTM's do on multilabeled classification tasks, and with tasks where the input text length varies, such as the one in the news dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc2d99b9d17c3b8326e5aaa51e8ab768ef640c8dd496033039049585146abff1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
